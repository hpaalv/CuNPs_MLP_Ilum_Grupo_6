{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Criatura Lendária 1 : Predição de Energia Total e de Formação de Nanopartículas de Cobre Utilizando Deeplearning\n",
    "\n",
    "## Trabalho de conclusão de curso da disciplina de Redes Neurais da Ilum Escola De Ciência, em que foi proposto a criação de uma rede neural MLP para algum tema relevante no meio científico.\n",
    "\n",
    "## Grupo 6: Anna Karen Pinto; Beatriz Borges; Paulo Henrique dos Santos\n",
    "Campinas/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introdução\n",
    "\n",
    "Materiais em escala nanométrica exibem características distintas em comparação com materiais em escalas macrométricas, devido a uma série de fatores, incluindo a superfície exposta dos materiais. Em escalas nanométricas, a relação entre área de superfície e volume é amplificada, tornando a superfície de contato proporcionalmente maior em relação ao volume do material. Essa proporção aumentada da superfície confere propriedades aos materiais nanoestruturados. [2].\n",
    "\n",
    "Os nanomateriais possuem uma variedade de aplicações, incluindo catálise, imagiologia por ressonância magnética e liberação controlada de fármacos. Além disso, processos de modificação superficial podem ser empregados para mitigar os efeitos citotóxicos associados a certos materiais. Essas modificações podem incluir revestimentos ou funcionalizações que tornam a interação com o ambiente biológico mais favorável, reduzindo assim os efeitos adversos. São classificadas como nanométricas partículas com dimensões tipicamente entre 1-100 nm. [2][3][4]. Portanto, é fundamental entendermos como a energia total e de formação influencia no produto final e nas características para a qual a nanopartícula será designada, permitindo a implementação de medidas preventivas e o planejamento adequado, incentivando um investimento tecnológico e científico maior nesta área.\n",
    "\n",
    "A ciência que é utilizada para esta problemática é a Rede Neural tipo MLP (multilayer perceptron) ou, em português, perceptron multicamadas, que é uma rede neural artificial moderna de alimentação direta (feedforward). Essa rede é composta por várias camadas, incluindo uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída.[5].\n",
    "\n",
    "A rede recebe os dados na camada de entrada com seus respectivos pesos. Cada neurônio possui uma função de ativação e um viez ao qual realizará cáculos. Durante o processo de aprendizado, os pesos de conexão na rede são ajustados após o processamento de cada dado com base na quantidade de erro na saída em comparação com o resultado esperado. O qual permite que um sistema aprenda e melhore de forma autônoma, sem ser programado explicitamente, alimentando-o com grandes quantidades de dados. [5]\n",
    "\n",
    "Esses dados podem ser coletados de diversas formas possíveis, variando de acordo com sua finalidade e recursos para a pesquisa. Entretanto, algo em comum com todo qualquer tipo de dado é que eles são armazenados em dataset.\n",
    "\n",
    "Dataset é um conjunto de dados estruturados em uma tabela, contendo descrições específicas de seus atributos e arquivos significativos para o conjunto. [6] Com o conjunto de dados, é possível extrair informações necessárias para a aplicação/manipulação desejada.\n",
    "\n",
    "A escolha desse tipo de dataset permite uma abordagem multidisciplinar, integrando conhecimentos, sem perder a importância científica proposta pelo trabalho final.\n",
    "\n",
    "Os códigos aqui apresetados estão organizados da seguinte forma:\n",
    "- Importação das bibliotecas necessárias\n",
    "- Obtenção e Tratamento dos dados\n",
    "- Criação e Aplicação da MLP\n",
    "- Avaliação dos Resultados e Conlusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importação das Bibliotecas\n",
    "\n",
    "Todas as bibliotecas aqui importadas foram essenciais para a realização do trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and Treat Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "#VIF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import datetime\n",
    "import pickle\n",
    "#NN\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Obtenção e Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, os seguintes passos foram tomados:\n",
    "\n",
    "- Importando o Dataset e transformando em variável\n",
    "- Separando em treino e teste\n",
    "- Aplicando o Logaritmo e Normalização pelo máximo absoluto\n",
    "- Aplicando o VIF\n",
    "\n",
    "Cada um desses passos possui sua importância que esta explícita logo acima da sua relização em código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importando o Dataset e transformando em variável:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>T</th>\n",
       "      <th>tau</th>\n",
       "      <th>time</th>\n",
       "      <th>N_total</th>\n",
       "      <th>N_bulk</th>\n",
       "      <th>N_surface</th>\n",
       "      <th>Volume</th>\n",
       "      <th>R_min</th>\n",
       "      <th>R_max</th>\n",
       "      <th>...</th>\n",
       "      <th>q6q6_S14</th>\n",
       "      <th>q6q6_S15</th>\n",
       "      <th>q6q6_S16</th>\n",
       "      <th>q6q6_S17</th>\n",
       "      <th>q6q6_S18</th>\n",
       "      <th>q6q6_S19</th>\n",
       "      <th>q6q6_S20</th>\n",
       "      <th>q6q6_S20+</th>\n",
       "      <th>Total_E</th>\n",
       "      <th>Formation_E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3281</td>\n",
       "      <td>923</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>42</td>\n",
       "      <td>91</td>\n",
       "      <td>1.570000e-27</td>\n",
       "      <td>4.737965</td>\n",
       "      <td>8.103804</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-385.20936</td>\n",
       "      <td>85.61064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3282</td>\n",
       "      <td>923</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>44</td>\n",
       "      <td>92</td>\n",
       "      <td>1.600000e-27</td>\n",
       "      <td>4.023433</td>\n",
       "      <td>8.361129</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-393.45040</td>\n",
       "      <td>87.98960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2291</td>\n",
       "      <td>723</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>51</td>\n",
       "      <td>98</td>\n",
       "      <td>1.750000e-27</td>\n",
       "      <td>4.571391</td>\n",
       "      <td>7.953961</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-441.89002</td>\n",
       "      <td>85.56998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2292</td>\n",
       "      <td>723</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>54</td>\n",
       "      <td>97</td>\n",
       "      <td>1.780000e-27</td>\n",
       "      <td>4.754535</td>\n",
       "      <td>8.400565</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-451.64457</td>\n",
       "      <td>82.89543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3091</td>\n",
       "      <td>823</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>67</td>\n",
       "      <td>103</td>\n",
       "      <td>2.000000e-27</td>\n",
       "      <td>5.673022</td>\n",
       "      <td>8.205280</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-506.59469</td>\n",
       "      <td>95.20531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>1210</td>\n",
       "      <td>523</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>19487</td>\n",
       "      <td>16311</td>\n",
       "      <td>3176</td>\n",
       "      <td>2.290000e-25</td>\n",
       "      <td>34.466223</td>\n",
       "      <td>44.887615</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65609.94800</td>\n",
       "      <td>3374.03200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2110</td>\n",
       "      <td>673</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>19507</td>\n",
       "      <td>16289</td>\n",
       "      <td>3218</td>\n",
       "      <td>2.300000e-25</td>\n",
       "      <td>34.247301</td>\n",
       "      <td>44.673954</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65392.19100</td>\n",
       "      <td>3662.58900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1010</td>\n",
       "      <td>423</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>19509</td>\n",
       "      <td>16181</td>\n",
       "      <td>3328</td>\n",
       "      <td>2.300000e-25</td>\n",
       "      <td>21.874439</td>\n",
       "      <td>46.705838</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66085.15400</td>\n",
       "      <td>2976.70600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>1910</td>\n",
       "      <td>573</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>19517</td>\n",
       "      <td>16255</td>\n",
       "      <td>3262</td>\n",
       "      <td>2.300000e-25</td>\n",
       "      <td>33.916720</td>\n",
       "      <td>48.546207</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65662.22800</td>\n",
       "      <td>3427.95200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2010</td>\n",
       "      <td>623</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>19591</td>\n",
       "      <td>16334</td>\n",
       "      <td>3257</td>\n",
       "      <td>2.310000e-25</td>\n",
       "      <td>34.388360</td>\n",
       "      <td>46.445668</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65879.70700</td>\n",
       "      <td>3472.43300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    T  tau  time  N_total  N_bulk  N_surface        Volume  \\\n",
       "0     3281  923   45     1      133      42         91  1.570000e-27   \n",
       "1     3282  923   45     2      136      44         92  1.600000e-27   \n",
       "2     2291  723   50     1      149      51         98  1.750000e-27   \n",
       "3     2292  723   50     2      151      54         97  1.780000e-27   \n",
       "4     3091  823   50     1      170      67        103  2.000000e-27   \n",
       "...    ...  ...  ...   ...      ...     ...        ...           ...   \n",
       "3995  1210  523    5    10    19487   16311       3176  2.290000e-25   \n",
       "3996  2110  673    5    10    19507   16289       3218  2.300000e-25   \n",
       "3997  1010  423    5    10    19509   16181       3328  2.300000e-25   \n",
       "3998  1910  573    5    10    19517   16255       3262  2.300000e-25   \n",
       "3999  2010  623    5    10    19591   16334       3257  2.310000e-25   \n",
       "\n",
       "          R_min      R_max  ...  q6q6_S14  q6q6_S15  q6q6_S16  q6q6_S17  \\\n",
       "0      4.737965   8.103804  ...         0         0         0         0   \n",
       "1      4.023433   8.361129  ...         0         0         0         0   \n",
       "2      4.571391   7.953961  ...         0         0         0         0   \n",
       "3      4.754535   8.400565  ...         0         0         0         0   \n",
       "4      5.673022   8.205280  ...         0         0         0         0   \n",
       "...         ...        ...  ...       ...       ...       ...       ...   \n",
       "3995  34.466223  44.887615  ...         0         0         0         0   \n",
       "3996  34.247301  44.673954  ...         0         0         0         0   \n",
       "3997  21.874439  46.705838  ...         0         0         0         0   \n",
       "3998  33.916720  48.546207  ...         0         0         0         0   \n",
       "3999  34.388360  46.445668  ...         0         0         0         0   \n",
       "\n",
       "      q6q6_S18  q6q6_S19  q6q6_S20  q6q6_S20+      Total_E  Formation_E  \n",
       "0            0         0         0          0   -385.20936     85.61064  \n",
       "1            0         0         0          0   -393.45040     87.98960  \n",
       "2            0         0         0          0   -441.89002     85.56998  \n",
       "3            0         0         0          0   -451.64457     82.89543  \n",
       "4            0         0         0          0   -506.59469     95.20531  \n",
       "...        ...       ...       ...        ...          ...          ...  \n",
       "3995         0         0         0          0 -65609.94800   3374.03200  \n",
       "3996         0         0         0          0 -65392.19100   3662.58900  \n",
       "3997         0         0         0          0 -66085.15400   2976.70600  \n",
       "3998         0         0         0          0 -65662.22800   3427.95200  \n",
       "3999         0         0         0          0 -65879.70700   3472.43300  \n",
       "\n",
       "[4000 rows x 185 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv', delimiter = ';')\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Separando em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS listas representadas abaixo são referentes, respectivamente, aos atributos e targets. Nas 3 primeiras linhas de código definimos essas listas, e nas 3 ultimas separamos entre treino e teste, sublistas que possibilitarão o treinamento e teste da rede neural. Ademais, é válido mencionar que alguns dos atributos possuíam valores zero em todos os seus dados, assim, realizamos uma seleção manual destes, e os descartamos. A lista criada abaixo já está livre destes atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição do que serão as features e os targets\n",
    "X = df[[\"T\", \"tau\", \"time\", \"N_total\", \"N_bulk\", \"N_surface\", \"Volume\", \"R_min\", \"R_max\", \"R_diff\", \"R_avg\", \"R_std\", \"R_skew\", \"R_kurt\", \"S_100\", \"S_111\", \"S_110\", \"S_311\", \"Curve_1-10\", \"Curve_11-20\", \"Curve_21-30\", \"Curve_31-40\", \"Curve_41-50\", \"Curve_51-60\", \"Curve_61-70\", \"Curve_71-80\", \"Curve_81-90\", \"Curve_171-180\", \"Avg_total\", \"Avg_bulk\", \"Avg_surf\", \"TCN_1\", \"TCN_2\", \"TCN_3\", \"TCN_4\", \"TCN_5\", \"TCN_6\", \"TCN_7\", \"TCN_8\", \"TCN_9\", \"TCN_10\", \"TCN_11\", \"TCN_12\", \"TCN_13\", \"TCN_14\", \"TCN_15\", \"TCN_16\", \"BCN_6\", \"BCN_7\", \"BCN_8\", \"BCN_9\", \"BCN_10\", \"BCN_11\", \"BCN_12\", \"BCN_13\", \"BCN_14\", \"BCN_15\", \"BCN_16\", \"SCN_1\", \"SCN_2\", \"SCN_3\", \"SCN_4\", \"SCN_5\", \"SCN_6\", \"SCN_7\", \"SCN_8\", \"SCN_9\", \"SCN_10\", \"SCN_11\", \"SCN_12\", \"SCN_13\", \"SCN_14\", \"Avg_bonds\", \"Std_bonds\", \"Max_bonds\", \"Min_bonds\", \"N_bonds\", \"angle_avg\", \"angle_std\", \"FCC\", \"HCP\", \"ICOS\", \"DECA\", \"q6q6_avg_total\", \"q6q6_avg_bulk\", \"q6q6_avg_surf\", \"q6q6_T0\", \"q6q6_T1\", \"q6q6_T2\", \"q6q6_T3\", \"q6q6_T4\", \"q6q6_T5\", \"q6q6_T6\", \"q6q6_T7\", \"q6q6_T8\", \"q6q6_T9\", \"q6q6_T10\", \"q6q6_T11\", \"q6q6_T12\", \"q6q6_T13\", \"q6q6_T14\", \"q6q6_B0\", \"q6q6_B1\", \"q6q6_B2\", \"q6q6_B3\", \"q6q6_B4\", \"q6q6_B5\", \"q6q6_B6\", \"q6q6_B7\", \"q6q6_B8\", \"q6q6_B9\", \"q6q6_B10\", \"q6q6_B11\", \"q6q6_B12\", \"q6q6_B13\", \"q6q6_B14\", \"q6q6_B15\", \"q6q6_B16\", \"q6q6_B17\", \"q6q6_B18\", \"q6q6_B19\", \"q6q6_B20\", \"q6q6_B20+\", \"q6q6_S0\", \"q6q6_S1\", \"q6q6_S2\", \"q6q6_S3\", \"q6q6_S4\", \"q6q6_S5\", \"q6q6_S6\", \"q6q6_S7\", \"q6q6_S8\", \"q6q6_S9\", \"q6q6_S10\", \"q6q6_S11\", \"q6q6_S12\", \"q6q6_S13\", \"q6q6_S20+\"]]\n",
    "y = df[[\"Total_E\", \"Formation_E\"]]\n",
    "atributos = [\"T\", \"tau\", \"time\", \"N_total\", \"N_bulk\", \"N_surface\", \"Volume\", \"R_min\", \"R_max\", \"R_diff\", \"R_avg\", \"R_std\", \"R_skew\", \"R_kurt\", \"S_100\", \"S_111\", \"S_110\", \"S_311\", \"Curve_1-10\", \"Curve_11-20\", \"Curve_21-30\", \"Curve_31-40\", \"Curve_41-50\", \"Curve_51-60\", \"Curve_61-70\", \"Curve_71-80\", \"Curve_81-90\", \"Curve_171-180\", \"Avg_total\", \"Avg_bulk\", \"Avg_surf\", \"TCN_1\", \"TCN_2\", \"TCN_3\", \"TCN_4\", \"TCN_5\", \"TCN_6\", \"TCN_7\", \"TCN_8\", \"TCN_9\", \"TCN_10\", \"TCN_11\", \"TCN_12\", \"TCN_13\", \"TCN_14\", \"TCN_15\", \"TCN_16\", \"BCN_6\", \"BCN_7\", \"BCN_8\", \"BCN_9\", \"BCN_10\", \"BCN_11\", \"BCN_12\", \"BCN_13\", \"BCN_14\", \"BCN_15\", \"BCN_16\", \"SCN_1\", \"SCN_2\", \"SCN_3\", \"SCN_4\", \"SCN_5\", \"SCN_6\", \"SCN_7\", \"SCN_8\", \"SCN_9\", \"SCN_10\", \"SCN_11\", \"SCN_12\", \"SCN_13\", \"SCN_14\", \"Avg_bonds\", \"Std_bonds\", \"Max_bonds\", \"Min_bonds\", \"N_bonds\", \"angle_avg\", \"angle_std\", \"FCC\", \"HCP\", \"ICOS\", \"DECA\", \"q6q6_avg_total\", \"q6q6_avg_bulk\", \"q6q6_avg_surf\", \"q6q6_T0\", \"q6q6_T1\", \"q6q6_T2\", \"q6q6_T3\", \"q6q6_T4\", \"q6q6_T5\", \"q6q6_T6\", \"q6q6_T7\", \"q6q6_T8\", \"q6q6_T9\", \"q6q6_T10\", \"q6q6_T11\", \"q6q6_T12\", \"q6q6_T13\", \"q6q6_T14\", \"q6q6_B0\", \"q6q6_B1\", \"q6q6_B2\", \"q6q6_B3\", \"q6q6_B4\", \"q6q6_B5\", \"q6q6_B6\", \"q6q6_B7\", \"q6q6_B8\", \"q6q6_B9\", \"q6q6_B10\", \"q6q6_B11\", \"q6q6_B12\", \"q6q6_B13\", \"q6q6_B14\", \"q6q6_B15\", \"q6q6_B16\", \"q6q6_B17\", \"q6q6_B18\", \"q6q6_B19\", \"q6q6_B20\", \"q6q6_B20+\", \"q6q6_S0\", \"q6q6_S1\", \"q6q6_S2\", \"q6q6_S3\", \"q6q6_S4\", \"q6q6_S5\", \"q6q6_S6\", \"q6q6_S7\", \"q6q6_S8\", \"q6q6_S9\", \"q6q6_S10\", \"q6q6_S11\", \"q6q6_S12\", \"q6q6_S13\", \"q6q6_S20+\"]\n",
    "\n",
    "# Split de treino e teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.1, random_state=10)\n",
    "y_treino = y_treino.dropna()      #Retirada de valores NAN\n",
    "X_treino = X_treino.dropna()      #Retirada de valores NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aplicando o Logaritmo e Normalização pelo máximo absoluto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi proposto em em sala com o Orientador que nosso dados precisariam passar por duas normalizações principais: logaritmica e pelo máximo absoluto. Entretanto, nossos dados possuíam muitos valores 0, e o resultado do logaritmo de 0 é uma indefinição matemática. Na biblioteca que utilizamos para realizar o nosso logaritmo(linhas de código comentadas abaixo), o resultado do log de 0 é um valor NaN. Por isso, estávamos encontrando problemas ao treinar a rede, uma vez que o dataset X possuía uma quantidade de dados diferente do Y. Dessa forma, o grupo decidiu por não utilizar o logaritmo.\n",
    "\n",
    "Assim, realizmaos apenas a normalização pelo máximo absoluto, que é um procedimento utilizado para escalar os valores de um conjunto de dados de forma que o valor máximo seja 1 (ou -1, dependendo do intervalo escolhido). O processo em si é bastante simples e envolve dividir todos os valores do conjunto de dados pelo valor máximo absoluto encontrado no conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[6]\n",
    "\n",
    "#X_treino_log = np.log(np.array(X_treino))\n",
    "#X_teste_log = np.log(np.array(X_teste))\n",
    "#X_treino_log = X_treino_log.dropna()\n",
    "\n",
    "max_abs_normalizador = MaxAbsScaler()     #Definindo o normalizador\n",
    "X_treino_log_normalizado = max_abs_normalizador.fit_transform(X_treino)     #Normalizando X\n",
    "X_teste_log_normalizado = max_abs_normalizador.fit_transform(X_teste)     #Normalizando o Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aplicando o VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso dataset era composto por muitos atributos, e uma possível mitigação para este problema seria aplicar um VIF, que significa \"Variance Inflation Factor\" (Fator de Inflação da Variância). É uma medida estatística usada para diagnosticar multicolinearidade em uma regressão linear. Multicolinearidade ocorre quando duas ou mais variáveis independentes em um modelo de regressão linear estão altamente correlacionadas, o que pode causar problemas na interpretação dos coeficientes de regressão e na precisão das previsões.[7]\n",
    "\n",
    "Os códigos deste algorítmo foram disponibilizados pelo professor, de um acervo próprio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_selection(\n",
    "    x: np.ndarray,\n",
    "    cols: list,\n",
    "    max_vif: float = 8,\n",
    "    deleted_cols=[],\n",
    "    savepath=None,\n",
    "    verbose=False,\n",
    "    initial_pass=False,\n",
    "):\n",
    "\n",
    "    min_tol = 1 / max_vif\n",
    "    idx = 0\n",
    "\n",
    "    def quick_tolerance_min_idx(x, idx=0):\n",
    "        tolerance = np.zeros(x.shape[1])\n",
    "\n",
    "        # to improve speed of finding the intial zeros\n",
    "        gen = deque(range(x.shape[1]))\n",
    "        gen.rotate(-idx)\n",
    "        gen = list(gen)\n",
    "\n",
    "        for i in gen:\n",
    "            X, y = np.delete(x, i, 1), x[:, i]\n",
    "            # https://stackoverflow.com/questions/36573046/difference-between-numpy-linalg-lstsq-and-sklearn-linear-model-linearregression\n",
    "            r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "            tol = 1 - r_squared\n",
    "            if tol == 0:\n",
    "                return i, tol\n",
    "            else:\n",
    "                tolerance[i] = tol\n",
    "\n",
    "        idx = np.argmin(tolerance)\n",
    "        return idx, tolerance[idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(len(cols), datetime.datetime.now())\n",
    "\n",
    "    def check(tol):\n",
    "        if initial_pass:\n",
    "            return tol == 0\n",
    "        else:\n",
    "            return tol < min_tol\n",
    "\n",
    "    while True:\n",
    "        idx, tol = quick_tolerance_min_idx(x, idx)\n",
    "        if check(tol):\n",
    "            x = np.delete(x, idx, 1)\n",
    "            poped_col = cols.pop(idx)\n",
    "            deleted_cols.extend([poped_col])\n",
    "            if savepath:\n",
    "                pickle.dump(deleted_cols, open(savepath, \"wb\"))\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    len(cols), datetime.datetime.now(), f\"{tol:.3g}\", poped_col\n",
    "                )\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return x, cols, deleted_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_vif = vif_selection(X_treino_log_normalizado, atributos)     #Aplicação do VIF em nossos dados\n",
    "X_final, cols, deleted_cols = X_norm_vif      #Separando o resultado do VIF em 3 dados: dataset final, colunas totais e colunas apagadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após todas estas etapas, nossos dados estão prontos para serem aplicados na MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Criação e Aplicação da MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a criação e aplicação da MLP utilizamos como base o caderno da aula de Torch, mininstrada pelo nosso Orientador. Assim, realizamos algumas mudanças para adaptar a MLP ao nosso caso.[8] \n",
    "\n",
    "Precisávamos de uma rede que variasse a quantidade de neurônios e de camadas, para que encontrássemos a melhor configuração possível para nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, num_dados_entrada, neuronios_camadas, num_targets):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []      # Lista para armazenar as camadas\n",
    "        \n",
    "        for i in range(len(neuronios_camadas) - 1): # Adicionando camadas intermediárias\n",
    "            layers.append(nn.Linear(neuronios_camadas[i], neuronios_camadas[i+1]))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        \n",
    "        layers.append(nn.Linear(neuronios_camadas[-1], num_targets))     # Adicionando camada de saída\n",
    "        \n",
    "        self.camadas = nn.Sequential(*layers)     # Definindo as camadas como uma sequência\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.camadas(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.tensor(X_final)     #Transformando nossos dados em tensores, formato utilizado pelo Pytorch\n",
    "y = torch.tensor(np.array(y_treino))\n",
    "y = y.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que  temos nossa rede criada, precisamos definir o intervalo de variação dos nosso hiperparâmetros, buscando uma alternativa que não seja muito custosa computacionalmente mas que ainda assim atinja nossas espectativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = x.shape[1]     #Numero de dados de entradas representado pela quantidade de colunas do dataset\n",
    "\n",
    "NUM_DADOS_DE_SAIDA = 2     #Número de dados de saída\n",
    "\n",
    "NEURONIOS = list(range(10, 80, 2))     #Intervalo da quantidade de neurônios\n",
    "\n",
    "CAMADAS = list(range(2, 6))     #Intervalo da quantidade de camadas\n",
    "\n",
    "TAXA_DE_APRENDIZADO = 0.01      #Definindo a Taxa de Aprendizado da MLP\n",
    " \n",
    "#otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)     #Definindo o Otimizador\n",
    "        \n",
    "fn_perda = nn.MSELoss()     #Definindo a função Perda\n",
    "        \n",
    "NUM_EPOCAS = 10000     #Definindo o número de épocas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os hiperparâmetros definidos, criamos um loop que irá iterar de forma a criar e testar várias redes, vairando a quantidade de camadas e neurônios como propomos. Para isso, criamos duas variáveis, uma que armazenará os hiperparâmetros(a rede sem si) e outra que armazenará o valor do MSE(função de perda adotada pelo grupo). Assim, sempre que uma nova rede for criada, o valor do melhor MSE dessa rede vai ser comparado com o armazenado na variável fixa, caso esse valor seja menor que o lá armazeado, ele o substituirá. Assim, a rede com o valor menor também irá substituir a rede já colocada na variável que armazena os hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor MSE: tensor(89030672., grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "melhor_mse = float('inf')     #Criando a variável do MSE\n",
    "melhor_rede = None     #Criando a variável dos hiperparâmetros\n",
    "\n",
    "for layer in CAMADAS:    #Criando o loop que itera as camadas\n",
    "    \n",
    "    for neuronios in NEURONIOS:     #Criando o loop que itera os neurônios\n",
    "        neuronios_camadas = [NUM_DADOS_DE_ENTRADA]     #Criando uma lista com o valor dos dados de entrada\n",
    "        \n",
    "        for a in range(layer):\n",
    "            neuronios_camadas.append(neuronios)     #Adicionando os neurônios em cada camada na quantidade atual do loop\n",
    "        \n",
    "        minha_mlp = MLP(     #Criando a instância da classe da MLP\n",
    "            NUM_DADOS_DE_ENTRADA, neuronios_camadas, NUM_DADOS_DE_SAIDA\n",
    "        )\n",
    "        \n",
    "        otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)     #Definindo o Otimizador\n",
    "       \n",
    "        minha_mlp.train()     #Definindo a ação para treino da MLP\n",
    "\n",
    "        for epoca in range(NUM_EPOCAS):      #Treinando a MLP, passando por cada passo\n",
    "            \n",
    "            y_pred = minha_mlp(x)     #Forward pass\n",
    "\n",
    "            otimizador.zero_grad()     #Zero grad\n",
    "\n",
    "            loss = fn_perda(y, y_pred)     #Loss\n",
    "            \n",
    "            loss.backward()     #Backpropagation\n",
    "\n",
    "            otimizador.step()     #Atualiza parâmetros\n",
    "\n",
    "            if loss < melhor_mse:     #Verifica se o MSE atual é melhor do que o melhor MSE registrado até agora\n",
    "                melhor_mse = loss\n",
    "                melhor_rede = minha_mlp\n",
    "\n",
    "with open('melhor_rede.pkl', 'wb') as f:     #Salva a melhor rede em um arquivo usando pickle\n",
    "    pickle.dump(melhor_rede, f)\n",
    "\n",
    "print(\"Melhor MSE:\", melhor_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação dos Resultados e Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Copper Nanoparticle Data Set. Disponível em: https://data.csiro.au/collection/csiro:42598. Acesso em: 09 abr. 2024.\n",
    "\n",
    "[2] OS NANOMATERIAIS E A DESCOBERTA DE NOVOS MUNDOS NA BANCADA DO QUÍMICO | Manuel A. Martins e Tito Trindade - Quim. Nova, Vol. 35, No. 7, 1434-1446, 2012. Disponível em: https://www.scielo.br/j/qn/a/P8tgywDnt7nS6tGyHdQ3BCF/. Acesso em: 02 mai. 2024.\n",
    "\n",
    "[3] Ojha, N. K.; Zyryanov, G. V.; Majee, A.; Charushin, V. N.; Chupakhin, O. N.; Santra, S. Copper nanoparticles as inexpensive and efficient catalyst: A valuable contribution inorganic synthesis. Coordination Chemistry Reviews 2017, 353, 1–57.11.\n",
    "\n",
    "‌[4] Ssekatawa K, Byarugaba DK, Angwe MK, Wampande EM, Ejobi F, Nxumalo E, Maaza M, Sackey J, Kirabira JB. Phyto-Mediated Copper Oxide Nanoparticles for Antibacterial, Antioxidant and Photocatalytic Performances. Front Bioeng Biotechnol. 2022 Feb 16;10:820218. doi: 10.3389/fbioe.2022.820218. PMID: 35252130; PMCID: PMC8889028.\n",
    "\n",
    "‌[5] Multilayer perceptron | Wikipedia, the free encyclopedia. Disponível em https://en.wikipedia.org/wiki/Multilayer_perceptron#:~:text=A%20multilayer%20perceptron%20(MLP)%20is,that%20is%20not%20linearly%20separable.. Acesso em: 29 abr. 2024.\n",
    "\n",
    "[6] https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\n",
    "\n",
    "[7] https://online.stat.psu.edu/stat462/node/180/\n",
    "\n",
    "[8] ATP-303 NN 5.2 - Notebook PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8529d5d23f5cbf0c063605dab2c59e7e62347018e6f532e027d68294076af1e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
